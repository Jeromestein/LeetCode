| Category   | Difficulty       | Likes | Dislikes |
| ---------- | ---------------- | ----- | -------- |
| algorithms | #Medium (49.36%) | 3432  | 111      |

**Tags**

[`depth-first-search`](https://leetcode.com/tag/depth-first-search?source=vscode "https://leetcode.com/tag/depth-first-search?source=vscode") | [`breadth-first-search`](https://leetcode.com/tag/breadth-first-search?source=vscode "https://leetcode.com/tag/breadth-first-search?source=vscode") | [`union-find`](https://leetcode.com/tag/union-find?source=vscode "https://leetcode.com/tag/union-find?source=vscode") | [`graph`](https://leetcode.com/tag/graph?source=vscode "https://leetcode.com/tag/graph?source=vscode") #union-find #bfs #graph #tree 

**Companies**

`facebook` | `google` | `zenefits`

You have a graph of `n` nodes labeled from `0` to `n - 1`. You are given an integer n and a list of `edges` where `edges[i] = [ai, bi]` indicates that there is an undirected edge between nodes `ai` and `bi` in the graph.

Return `true` _if the edges of the given graph make up a valid tree, and_ `false` _otherwise_.

**Example 1:**

![](https://assets.leetcode.com/uploads/2021/03/12/tree1-graph.jpg)

```
Input: n = 5, edges = [[0,1],[0,2],[0,3],[1,4]]
Output: true
```

**Example 2:**

![](https://assets.leetcode.com/uploads/2021/03/12/tree2-graph.jpg)

```
Input: n = 5, edges = [[0,1],[1,2],[2,3],[1,3],[1,4]]
Output: false
```

**Constraints:**

- `1 <= n <= 2000`
- `0 <= edges.length <= 5000`
- `edges[i].length == 2`
- `0 <= ai, bi < n`
- `ai != bi`
- There are no self-loops or repeated edges.

---

[Submissions](https://leetcode.com/problems/graph-valid-tree/submissions/?source=vscode "https://leetcode.com/problems/graph-valid-tree/submissions/?source=vscode") | [Solution](https://leetcode.com/problems/graph-valid-tree/solutions/?source=vscode "https://leetcode.com/problems/graph-valid-tree/solutions/?source=vscode")

BFS
```python

class Solution:
    def validTree(self, n: int, edges: List[List[int]]) -> bool:
        if len(edges)!=n-1:
            return False

        visited=set()
        edge_map=defaultdict(list)
        for e in edges:
            a,b=e[0],e[1]
            edge_map[a].append(b)
            edge_map[b].append(a)

        q=deque()
        q.append(0)

        while q:
            # print(q)
            curr_node=q.popleft()
            if curr_node in visited:
                continue

            visited.add(curr_node)

            for nxt in edge_map[curr_node]:
                q.append(nxt)

        # print(edge_map)

        return len(visited)==n

```
O(n+E)
O(n+E)

DSU:
```python

class UnionFind:
    
    # For efficiency, we aren't using makeset, but instead initialising
    # all the sets at the same time in the constructor.
    def __init__(self, n):
        self.parent = [node for node in range(n)]
        # We use this to keep track of the size of each set.
        self.size = [1] * n
        
    # The find method, with path compression. There are ways of implementing
    # this elegantly with recursion, but the iterative version is easier for
    # most people to understand!
    def find(self, A):
        # Step 1: Find the root.
        root = A
        while root != self.parent[root]:
            root = self.parent[root]
        # Step 2: Do a second traversal, this time setting each node to point
        # directly at A as we go.
        while A != root:
            old_root = self.parent[A]
            self.parent[A] = root
            A = old_root
        return root
        
    # The union method, with optimization union by size. It returns True if a
    # merge happened, False if otherwise.
    def union(self, A, B):
        # Find the roots for A and B.
        root_A = self.find(A)
        root_B = self.find(B)
        # Check if A and B are already in the same set.
        if root_A == root_B:
            return False
        # We want to ensure the larger set remains the root.
        if self.size[root_A] < self.size[root_B]:
            # Make root_B the overall root.
            self.parent[root_A] = root_B
            # The size of the set rooted at B is the sum of the 2.
            self.size[root_B] += self.size[root_A]
        else:
            # Make root_A the overall root.
            self.parent[root_B] = root_A
            # The size of the set rooted at A is the sum of the 2.
            self.size[root_A] += self.size[root_B]
        return True

class Solution:
    def validTree(self, n: int, edges: List[List[int]]) -> bool:
        # Condition 1: The graph must contain n - 1 edges.
        if len(edges) != n - 1: 
	        return False
        
        # Create a new UnionFind object with n nodes. 
        unionFind = UnionFind(n)
        
        # Add each edge. Check if a merge happened, because if it 
        # didn't, there must be a cycle.
        for A, B in edges:
            if not unionFind.union(A, B):
                return False
        
        # If we got this far, there's no cycles!
        return True
```

Complexity Analysis

Let E be the number of edges, and N be the number of nodes.

α(N) is the Inverse Ackermann Function.

Time Complexity : O(N⋅α(N)).

When E

=N−1, we simply return false. Therefore, the worst case is when E=N−1. Because E is proportional to N, we'll say E=N to simplify the analysis.

We are putting each of the N edges into the UnionFind data structure, using the union(...) method. The union(...) method itself has no loops or recursion, so the entire cost of calling it is dependent on the cost of the find(...) method that it calls.

find(...)'s cost is dependent on how far the node it was searching for is from the root. Using the naïve implementation of union find, this depth could be N. If this was the case for all of the calls, we'd have a final cost of O(N 
2
 ).

However, remember those optimizations we did? Those keep the tree depths very shallow. It turns out that find(...) amortizes to O(α(N)), where α is the Inverse Ackermann Function. The incredible thing about this function is that it grows so slowly that N will never go higher than 4 in the universe as we know it! So while in "practice" it is effectively O(1), in "theory" it is not.

Actually proving this upper bound on the depth is a very advanced proof, which I'd certainly hope you'd never need to do in an interview! If you're interested though, I recommend looking in a good algorithm's text book or paper.

Anyway, this gives us a total of N⋅O(α(N))=O(N⋅α(N)).

Space Complexity : O(N).

The UnionFind data structure requires O(N) space to store the arrays it uses.

So, why is this better than Approach 2?

Complexity analysis ignores constants. For example, O(10⋅N)=O(N). Even O(10000⋅N)=O(N). Sometimes the constants we're ignoring in the analysis are still having a big impact on the run-time in practice.

Approach 2 had a lot of overhead in needing to create an adjacency list with the edges before it could even begin the depth-first search. This is all treated as a constant, as it ultimately had the same time complexity as the depth-first search itself.

Approach 3 doesn't need to change the input format, it can just get straight to determining whether or not there is a cycle. Additionally the bit that stops it being constant, the α(N), will never have a value larger than 4. So in practice, it behaves as a constant too—and a far smaller one at that!

When weighing up the pros and cons of different algorithms for solving problems, it's best to treat union find's operations as O(1) to get a fair and accurate comparison.